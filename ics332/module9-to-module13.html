<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>ICS 332 - Module 9 to Module 13</title>
  <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/showdown/1.9.0/showdown.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js"></script>
</head>
<body>
<a href="../index.html">Back to Home Page</a>
<i>This note was last edited on May/05/2019 09:03 PM</i>

<div id="target"></div>
<script>
  //Module 9
  const converter = new showdown.Converter();
  // Address Virtualization
  const note1 = '# 9. Main Memory: Address Virtualization\n' +
      '## Address Virtualization\n' +
      '* Main Memory = **Memory Unit** (in Von Neumann model)\n' +
      '      * (Large) contiguous array of bytes/words, each with its own address\n' +
      '      * Stream of addresses coming in on the **memory bus**\n' +
      '      * Each incoming address is stored in the memory-address register of the memory unit\n' +
      '      * Which causes the memory unit to put the content at that address on the memory bus\n' +
      '      * And that content is then read in by the CPU\n' +
      '* Called the “Main” memory by contrast with registers, caches, which are all managed 100% by the hardware\n' +
      '* **Processes share the main memory, therefore the OS must manage the main memory**\n' +
      '* The CPU only works with registers, but it can **issue addresses** that correspond to words of the main memory via *load/store instructions*\n' +
      '\n' +
      '## Contiguous Memory Allocation - Early Systems\n' +
      '* Let\'s assume what we have always assumed so far: each process is allocated a *contiguous* zone of physical memory\n' +
      '\n' +
      '![](@attachment/ics332-9-1.png)\n' +
      '\n' +
      '![](@attachment/ics332-9-2.png)\n' +
      '\n' +
      '## Address Binding\n' +
      '* One important question is that of **address binding**: *when are physical addresses determined for bytes of data/instruction*?\n' +
      '* Consider the following program:\n' +
      '\n' +
      '![](@attachment/ics332-9-3.png)\n' +
      '\n' +
      '* WHen is the address of where *a* is located determined/fixed?\n' +
      '* When is the address of the instruction to which to jump in case a is\n' +
      'zero?\n' +
      '\n' +
      '### Absolute Addressing\n' +
      '\n' +
      '![](@attachment/ics332-9-4.png)\n' +
      '\n' +
      '* The assembler transofmrs the assembly code into a binary executable\n' +
      '* One approach to address binding is to use **absolute addressing** so that the binary executable contains physical addresses:\n' +
      '\n' +
      '![](@attachment/ics332-9-5.png)\n' +
      '\n' +
      '#### Problems of Absolute Addressing\n' +
      '* Absolute addressing is simple, but it has **not** been used in decades\n' +
      '* **With absolute addressing a program must be loaded exactly at the same place into memory each time we run it**\n' +
      '* Therefore, we may not be able to run a program because another program is running and encoraches on the address range.\n' +
      '* **Corollary: We cannot run multiple instances of a single program!**\n' +
      '* One solution is to recompile a program each time you need to run it\n' +
      '      * Because only when you\'re about to run a program can you know where it should fit in memory\n' +
      '      * But this has problems: while you\'re recompiling it, somebody else starts another program and that program takes 1 hour to compile\n' +
      '* **Bottom line: absolute addressing is not a good idea**\n' +
      '\n' +
      '### Relative Addressing\n' +
      '* We can solve the problem of absolute addressing with a very simple idea called **relative addressing**\n' +
      '* Assume the address space starts at some **BASE address**, and computes all addresses as an **offset** from the BASE\n' +
      '* The code is now completely **relocatable**: Only the BASE needs to be determined before running it\n' +
      '* The same program can be run anywhere in memory (at whatever BASE address)\n' +
      '* Multiple instances can run, each with a different BASE address\n' +
      '\n' +
      '## Memory Virtualization\n' +
      '* From the process’ point of view, the address space starts at 0\n' +
      '* All addresses in the process address space are expressed as an **offset relative** to the **base** value\n' +
      '* The process address space has been **virtualized**\n' +
      '* An address in the **physical memory** is called a **physical address**\n' +
      '* The address manipulated by the **CPU** is a **logical address** or a virtual address (both terms are used)\n' +
      '* A program references a **logical address space**, which corresponds to a **physical address space** in the memory\n' +
      '* However “something” needs to tell the CPU how to translate from virtual to physical addresses, i.e., some **address translation** mechanism\n' +
      '\n' +
      '## Virtualizing the process address space\n' +
      '* Some hardware component needs to **translate** virtual addresses into physical addresses\n' +
      '* *Address translation happens very frequently (each load, store, jump)*\n' +
      '* Therefore: The BASE Address is accessed very frequently\n' +
      '      * The memory translation component should store it in a register or something as fast as a register\n' +
      '* And: Offsets are added to the BASE address very frequently\n' +
      '* Furthermore: It would be nice if only valid logical addresses were translated\n' +
      '      * For **memory protection**: we still don’t want processes to step on each other’s toes\n' +
      '* So we use a **limit register** that stores the largest possible logical address\n' +
      '* Conclusion: This component must be a specialized, super fast **hardware** component\n' +
      '\n' +
      '## Memory Management Unit\n' +
      '* That component that translates virtual addresses into physical addresses is named the **Memory Management Unit (MMU)** which is nowadays integrated with the CPU\n' +
      '\n' +
      '![](@attachment/ics332-9-6.png)\n' +
      '\n' +
      '## Summary of Address Virtualization\n' +
      '* Your program generates only **logical** addresses between 0 and some upper bound (the limit)\n' +
      '* Each such address is **checked** to see if it’s beyond the limit\n' +
      '* If not, then the address is **translated** (just add it to the base)\n' +
      '* That translated **physical address** is then sent to the memory bus\n' +
      '* Bottom line: *your CPU only “sees” logical addresses, your RAM only “sees” physical addresses*\n' +
      '\n' +
      '## Segmentation\n' +
      '* **Segmentation** is avoiding waste by breaking up the address space into pieces (each piece has its own base/limit register)\n' +
      '* The logical address space is now a collection of segments\n' +
      '\n' +
      '### Segment Table\n' +
      '* A **segment table** with one entry per segment number is used to keep track of segments\n' +
      '* For each segment, its entry stores:\n' +
      '      * Base: Starting address of the segment\n' +
      '      * Limit: Length of the segment\n' +
      '* **The segment table is stored in memory**\n' +
      '      * A Segment-Table Base Register (STBR): Points to the segment table address\n' +
      '      * A Segment-Table Length Register (STLR): Gives the length of the segment table, Makes it easy to detect an invalid segment offset\n' +
      '      * These registers are saved/restored at each context switch\n' +
      '      \n' +
      '### Memory Management Unit and Segmentation\n' +
      '* Implementing Segmentation is easy, Reserve bits (e.g., the left-most ones) in the logical address to reference a segment (**the segment bits**)\n' +
      '* Lookup the segment table (not shown on the picture) to find out the segment’s base/limit values\n' +
      '\n' +
      '![](@attachment/ics332-9-7.png)\n' +
      '\n' +
      '## Conclusion\n' +
      '* Main concept: the CPU sees logical addresses, and the MMU transforms them into physical addresses\n' +
      '      * Determines the segment\n' +
      '      * Lookup the segment table to find the segment’s base and limit\n' +
      '      * Check that the logical address is within the limit\n' +
      '      * Add the base to it\n';
  const html1 = converter.makeHtml(note1);
  const target = document.getElementById('target');
  target.innerHTML = html1;

  // Swapping
  const note2 = '\n' +
      '\n' +
      '# 9. Main Memory: Swapping\n' +
      '## Swapping\n' +
      '* Moving processes back and forth between main memory and the disk is called **swapping**\n' +
      '* When a process is swapped back in, it may be put into the same physical memory space *or not*\n' +
      '* With swapping a process can “be in RAM” or “be on Disk”\n' +
      '* Therefore, a context-switch can involve the disk (goes from fast to super slow)\n' +
      '​\n' +
      '### Swapping and DMA\n' +
      '* With swapping, at any time the OS could kick a process out of RAM and save it to disk\n' +
      '* This raises a concern with Direct Memory Access (DMA)\n' +
      '      * Reminder: with DMA a process says to the system “while I am doing other things please have the memory system do some memory copy without my involvement”\n' +
      '* Consider a process that has initiated a DMA operation and is swapped to disk\n' +
      '* The DMA controller may have no idea and happily continues to write data (into some other process’ address space, which has replaced that of the one that was swapped out!)\n' +
      '* Operating systems must deal with this (because DMA is so useful we can’t leave without it)\n' +
      '​\n' +
      '#### Bad news about swapping\n' +
      '* The disk is **super slow** (even if its an SSD)\n' +
      '* One approach is to just **not swap** (it is often disabled in laptops)\n' +
      '* A key solution is to not swap whole address spaces ("paging")\n' +
      '​\n' +
      '#### Where are we?\n' +
      '* We now have a whole bunch of *mechanisms*:\n' +
      '      * We know how to give each process a “slab” of memory or one slab per segment\n' +
      '      * We know how to control access to that memory\n' +
      '      * We know how to reduce address spaces\n' +
      '      * We know how to swap processes in and out of memory\n' +
      '* We now need a *policy* to decide how to place each slab in memory:\n' +
      '      * We want to have as many process address spaces in memory as possible\n' +
      '      * We want to minimize swapping\n' +
      '      \n' +
      '## Memory Partitioning\n' +
      '* Main question: Where should the processed be placed in memory?\n' +
      '* The kernel must keep a list of available memory regions or “holes”\n' +
      '* When a process arrives, before scheduling it, it is placed in a “I need memory” input queue\n' +
      '* The kernel must make decisions:\n' +
      '      * Pick a process from the input queue\n' +
      '      * Pick a hole in which the process will be placed (and update the list of holes)\n' +
      '* And then, the process can be placed in the ready queue\n' +
      '* This problem is known as the **dynamic storage allocation problem**\n' +
      '      * It’s an on-line problem (we don’t know the future)\n' +
      '      * As opposed to off-line (we know the future)\n' +
      '* **Objective: Hold as many processes in RAM as possible**\n' +
      '​\n' +
      '![](@attachment/ics332-9-8.png)\n' +
      '​\n' +
      '## Memory Allocation Strategies\n' +
      '* Question 1/3: Which process should be picked?\n' +
      '* Question 2/3: Which hole should be picked?\n' +
      '      * First Fit? Pick the first hole that is big enough\n' +
      '      * Best Fit? Pick the smallest hole that is big enough\n' +
      '      * Worst Fit? Pick the biggest hole\n' +
      '* Question 3/3: How should the picked process be placed in the picked hole? Top? Bottom? Middle?\n' +
      '* What should we do?\n' +
      '      * FCFS + First Fit + Top?\n' +
      '      * Jump Ahead + Worst Fit + Bottom?\n' +
      '      * The above combinations are **heuristics** that hopefully produce decent solutions\n' +
      '* What we are trying to solve is an on-line (don\'t know the future) bin-packing (fit boxes in bins) dynamic (boxes can disappear) problem!\n' +
      '​\n' +
      '## External Fragmentation\n' +
      '* Our objective: hold as many processes as possible in memory\n' +
      '* External fragmentation makes this difficult\n' +
      '* Two small disjoint holes that together would have been big enough to accodomate a process\n' +
      '* External fragmentation is defined as the number of holes\n' +
      '* What about **compaction**?\n' +
      '      * Just like defragging hard drives\n' +
      '      * But moving processes around means a lot of slow memory copies\n' +
      '      * No OS does it\n' +
      '​\n' +
      '## Internal Fragmentation\n' +
      '* When a process does not use a whole slab and some space is wasted\n' +
      '​\n' +
      '![](@attachment/ics332-9-9.png)\n' +
      '​\n' +
      '## Conclusion\n' +
      '* Our objective was to allocate a contiguous slab of memory to each process (or to each process segment) so that their address spaces can be in RAM\n' +
      '* The mechanisms are “easy”\n' +
      '      * Relocatable code\n' +
      '      * Segmentation for avoiding waste and enabling protection\n' +
      '      * MMU for address virtualization at runtime\n' +
      '* But finding a good policy is really hard\n' +
      '      * For process picking, hole picking, placement in hole\n' +
      '* It’s hard because fragmentation is unavoidable and wastes RAM\n' +
      '* One way to make it less hard is to try to have small address spaces\n' +
      '​\n';
  const html2 = converter.makeHtml(note2);
  target.innerHTML += html2;

  // Dynamic Linking ane Loading
  const note3 = '# 9. Main Memory: Dynamic Linking and Loading\n' +
      '## Smaller Address Space\n' +
      'Three common-place techniques for achieving smaller address spaces:\n' +
      '* Dynamic Memory Allocation\n' +
      '      * Ask programs to tell the OS exactly how much memory they need (malloc, new) so that we don\'t always allocate the maximum allowed RAM to each process\n' +
      '* Dynamic Loading\n' +
      '* Dynamic Linking\n' +
      '\n' +
      '## Dynamic Loading\n' +
      '* **Dynamic loading**: only load code/text when it’s needed\n' +
      '* Dynamic loading is the **programmer responsibility** (The OS is not involved)\n' +
      '* Supported in all (decent) programming languages / OSes:\n' +
      '* Note: We talk of dynamic loading but not of static loading (the\n' +
      'default behavior)\n' +
      '\n' +
      '## Static / Dynamic Linking\n' +
      '* Static Linking is the historical way of reusing code\n' +
      '* Add the assembly code of useful functions (printf...) collected in an **archive** or **library** to your own executable.\n' +
      '* Issue 1: Large text\n' +
      '* Issue 2: Some code is (very likely) duplicated in memory\n' +
      '* Key idea: Why not share text (i.e., code) between processes in a similar way as data can be shared through shared memory?\n' +
      '\n' +
      '### Dynamic Linking\n' +
      '* **The OS is in charge of loading the code** (but the OS needs to be told where to load it at compile time)\n' +
      '* The code is shared in **shared libraries**\n' +
      '      * libc.so for Linux (so = shared object)\n' +
      '      * MSVCRT.DLL for WIndows (DLL = Dynamic-link Library)\n' +
      '      \n' +
      '## Shared Libraries\n' +
      '* When dynamic linking is enabled, the linker just puts a **stub** in the binary for each shared library routine reference\n' +
      '* That **stub** is a piece of code that:\n' +
      '      * checks whether the routine is loaded in memory\n' +
      '      * if not, then loads it into memory “shared” (with all processes)\n' +
      '      * then replaces itself with a simple call to the routine (it’s self-modifying code!)\n' +
      '      * future calls will be “for free”\n' +
      '* This is how you can update your system and not have to recompile all your executables\n' +
      '* Shared libraries can also be replaced by a new one by overriding it\n' +
      '\n' +
      '## Conclusion\n' +
      '* Making address spaces as small as possible is thus a good idea\n' +
      '      * Won\'t have to swap as much\n' +
      '      * Not as costly to swap when swapping is needed\n' +
      '* Part of this is developer\'s responsibility:\n' +
      '      * Using space-efficient data structures\n' +
      '      * Use Dynamic Memory Allocation\n' +
      '* Part of this is provided by languages/compilers/OS\n' +
      '      * Dynamic loading\n' +
      '      * Dynamic linking';
  const html3 = converter.makeHtml(note3);
  target.innerHTML += html3;


  // Module 10
  // COunting and Addressing
  const note4 = '# 10. Counting and Addressing\n' +
      '## Units of Storage\n' +
      '* 1 Byte = 8 bits\n' +
      '* 1 KiB = \\\\( 2^{10} \\\\) Byte = 1,024 bytes\n' +
      '* 1 MiB = \\\\( 2^{10} \\\\) KiB = \\\\( 2^{20} \\\\) bytes ( 1 Million) (mega)\n' +
      '* 1 GiB = \\\\( 2^{10} \\\\) MiB = \\\\( 2^{30} \\\\) bytes ( 1 Billion) (giga)\n' +
      '* 1 TiB = \\\\( 2^{10} \\\\) GiB = \\\\( 2^{40} \\\\) bytes ( 1 Trillion) (tera)\n' +
      '* 1 PiB = \\\\( 2^{10} \\\\) TiB = \\\\( 2^{50} \\\\) bytes ( 1,000 Trillion) (peta)\n' +
      '* 1 EiB = \\\\( 2^{10} \\\\) PiB = \\\\( 2^{60} \\\\) bytes ( 1 Million Trillion) (exa)\n' +
      '\n' +
      '## Exponents, Logarithms\n' +
      '* \\\\( 2^x \\cdot 2^y = 2^{x+y} \\\\)\n' +
      '* \\\\( 2^{-x} = \\frac{1}{2^x}\\\\)\n' +
      '* \\\\( 2^x / 2^y = 2^{x-y} \\\\)\n' +
      '* \\\\( log_{2}2^n = n\\\\)\n' +
      '\n' +
      '## Addressing\n' +
      '* How many groups of *x* thingies are there in a set of *y* thingies? **y / x**\n' +
      '* We often partition thingies into chunks\n' +
      '* After partitioning, we need to **address** the chunks\n' +
      '* Addressing means to refer to something using a name/number\n' +
      '* **If you have \\\\( 2^n \\\\) thingies, then you need *n*-bit addresses to address the thingies**';
  const html4 = converter.makeHtml(note4);
  target.innerHTML += html4;


  // Module 11
  // Paging I
  const note5 = '';
  const html5 = converter.makeHtml(note5);
  target.innerHTML += html5;

  // Paging II
  const note6 = '';
  const html6 = converter.makeHtml(note6);
  target.innerHTML += html6;

  // Paging III (Page Faults and Replacement)
  const note7 = '# 11. Page Faults and Replacement\n' +
      '## Demand Paging\n' +
      '* The way in which the OS allocates pages to a process is called **Demand Paging**\n' +
      '* “Don’t load a page before the process references it”\n' +
      '      * Initially just load one page, the one with the first instruction of the program\n' +
      '      * Each time the program issues an address, load the corresponding page if not already loaded\n' +
      '* This is a “lazy” scheme, as opposed to the “eager” scheme that would load all pages at once\n' +
      '* For each page, the OS keeps track of whether it is in RAM or not\n' +
      '* This is done using the valid bit of the page table entries\n' +
      '      * A page is marked as valid if it is legal and in memory\n' +
      '      * A page is marked as invalid if it is illegal or on disk\n' +
      '      * Initially all pages are marked invalid\n' +
      '* During address translation, if the bit is invalid, a **trap** is generated: a **page fault**\n' +
      '\n' +
      '## Page Faults\n' +
      '* When the CPU issues an address, first one determines whether it’s legal or not\n' +
      '* Lookup the valid bit in the page table entry\n' +
      '* If the valid bit is set, do the address translation as usual\n' +
      '* If not:\n' +
      '      * Find a free frame (from the list of free frames in the kernel)\n' +
      '      * Schedule the disk access to load the page into the frame\n' +
      '      * Kick the process off the CPU and put it the blocked/waiting state\n' +
      '      * Once the disk access is complete, update the process page table with the new logical/physical memory mapping\n' +
      '      * Update the valid bit\n' +
      '      * Rerun the instruction that caused the trap\n' +
      '      * Set the process state to Ready (it should then run soon)\n' +
      '      \n' +
      '      \n' +
      '## Rerun the "offending" instruction\n' +
      '* If the page fault was because the instruction could not be fetched: (1) load the page, then (2) rerun the instruction from scratch\n' +
      '* If the page fault was because an operand value could not be read from memory: Do the same sequence\n' +
      '* If the trap been was issued because an operand value could not be written to memory: Do the same sequence\n' +
      '* In all cases: just re-run the instruction from scratch\n' +
      '\n' +
      '## Virtual Memory Performance\n' +
      '\n' +
      '![](@attachment/ics332-11-1.png)\n' +
      '\n' +
      '![](@attachment/ics332-11-2.png)\n' +
      '\n' +
      '**The page fault rate must be kept as small as possible**\n' +
      '\n' +
      'What can be done?\n' +
      '* Increase the memory size\n' +
      '* Limit the size of the process address space\n' +
      '* Tell programmers to develop programs with small address spaces ⇒ That’s your job! (every time you use more ram, you increase your\n' +
      'page fault probability, and thus slow down your program)\n' +
      '\n' +
      '#### Back to fork()-exec()\n' +
      '* We have said that fork() makes a copy of the parent process address space to create an identical child process\n' +
      '* But most of the time exec() is used in the child to run another program\n' +
      '\n' +
      '![](@attachment/ics332-11-3.png)\n' +
      '\n' +
      '### Copy-on-Write\n' +
      '* During a fork(), don’t copy the address space and initially share all pages\n' +
      '      * Save for some heap and stack pages, that are necessary for any new process\n' +
      '* Whenever the parent or the child modifies a page, then copy it\n' +
      '* This “lazy” scheme is used in all OSes (Windows, Mac, Linux)\n' +
      '* In the fork-exec classical example, no page is copied!\n' +
      '\n' +
      '## Page Replacement\n' +
      '* Virtual Memory increases multi-programming and provides the illusion of large address spaces\n' +
      '* What if we run out of memory?\n' +
      '      * A page fault occurs\n' +
      '      * Oh no, the free-frame list is empty!!\n' +
      '* *We need to kick a page out of RAM*; This is called **page replacement**\n' +
      '      * **Evict** a **victim** page from a frame (Write it to the disk if necessary)\n' +
      '      * Put the newly needed page into that frame\n' +
      '* Page replacement may thus require two page transfers\n' +
      '\n' +
      '## Dirty Bit\n' +
      '* **No need to write a victim back to disk if that victim has never been modified**\n' +
      '* For this reason, each page table entry has a dirty bit\n' +
      '* This dirty bit is initially set to 0\n' +
      '* Whenever the process writes to the page, that dirty bit is set to 1\n' +
      '* If a page is evicted, it’s written to disk only if its **dirty**\n' +
      '* Most OSes do opportunistic un-dirtying: If the disk is idle pick a dirty page, write it out and clear its dirty bit\n' +
      '\n' +
      '## Conclusion\n' +
      '* At this point we have *mechanisms*\n' +
      '      * We can bring pages in from disk on demand (when page fault)\n' +
      '      * We can write pages to disk when needed (RAM is full)\n' +
      '      * The dirty bit is used to avoid doing redundant writes to disk\n' +
      '* What we need are *policies*\n' +
      '* The main questions are: Which pages do we kick back to disk? and How many frames do we let a process have?';
  const html7 = converter.makeHtml(note7);
  target.innerHTML += html7;

  // Paging IV
  const note8 = '';
  const html8 = converter.makeHtml(note8);
  target.innerHTML += html8;


  // Module 12
  // Distributed Computing
  const note9 = '# 12. Introduction to Distributed Computing\n' +
      '## Distributed Computing Issue #1: Correctness\n' +
      '* Goal: Design an algorithm so that multiple processes accomplish a common goal **correctly**\n' +
      '      * By exchanging the right messages at the right times\n' +
      '* We need an algorithm that each process runs, that terminates, and that is correct\n' +
      '* We could design a **centralized** system: there is a special process that knows about all other processes (e.g., some server running somewhere)\n' +
      '      * Easy algorithm: just ask the special process who the leader should be\n' +
      '* But centralized systems have problems (e.g., what if the special process fails???)\n' +
      '* So we need a **distributed** algorithm...\n' +
      '* If every process in the system knows about all other processes it’s easy: pick the process with the lowest ID as\n' +
      'the leader!\n' +
      '* But typically, **processes don’t know the full picture** \n' +
      '      * The full picture is too big\n' +
      '      * The full picture changes all the time\n' +
      '* In general, maintaining global knowledge about the system doesn’t scale\n' +
      '\n' +
      '## Distributed Computing Issue #2: Fault-Tolerance\n' +
      '* When you write a program that runs on a single computer, and that computer crashes, you declare it a catastrophe that’s not your fault\n' +
      '* But a (good) distributed program should handle its processes unexpectedly dying/disappearing because that’s a common occurrence at large scale\n' +
      '* **First Question**: how do you detect failures?\n' +
      '      * A process hasn’t answered for a while... Is is dead?\n' +
      '      * Or is this just a temporary network problem and it will\n' +
      '      come back?\n' +
      '      * How long do I wait for?\n' +
      '      * And then if I’ve created a replacement process and that\n' +
      '      other process comes back, what do I do?\n' +
      '* **Second Question**: how do you handle failures?\n' +
      '* Perhaps you can duplicate processes, so that if one fails the other one survives\n' +
      '* But then these processes must be in perfect synchrony, which may be impossible, especially over a network\n' +
      '* Or you have a clever algorithm that adapts to failures gracefully, but that’s difficult\n' +
      '* You can probably envision how hard this is!\n' +
      '* Bottom Line: When you add failures into the mix, things are even more difficult\n' +
      '\n' +
      '## Distributed Computing Issue #3: Performance\n' +
      '* Say you want to perform a big/long computation with a lot of data\n' +
      '* You then ”enlist” computers and databases\n' +
      '      * You have some data on your laptop\n' +
      '      * There is some data in various databases on-line\n' +
      '      * You have accounts on compute and storage machines in various places\n' +
      '      * All interconnected via the network\n' +
      '* Problem: It’s not because you throw a bunch of hardware resources at your application that it will go faster\n' +
      '      * Networks are not infinitely fast\n' +
      '      * Not all computers are equal\n' +
      '      * Computation cannot run without data\n' +
      '      * Perhaps using a fast, but too remote computer, is not worth it\n' +
      '* Bottom Line: You have to orchestrate data transfers and computations carefully to get good performance\n' +
      '      * These are ”parallel computing” problems that people in HPC (High Performance Computing) study\n' +
      '\n';
  const html9 = converter.makeHtml(note9);
  target.innerHTML += html9;

  // Module 13
  // Mass Storage
  const note10 = '# 13. Mass Storage\n' +
      '## Magnetic Disks\n' +
      '* Magnetic disks (hard drives) are still the most common secondary storage devices today\n' +
      '* They are "messy" (errors, bad blocks, missed seeks, moving parts)\n' +
      '* The OS used to hide all the messiness from higher-level software (since programs shouldn\'t have to know anything about disk structure)\n' +
      '\n' +
      '![](@attachment/ics332-13-1.png)\n' +
      '\n' +
      '## Hard Drive Data Access\n' +
      '* A hard drive needs several pieces of information for each data access\n' +
      '      * Platter #, track #, sector #, etc.\n' +
      '* Nowadays, hard drives comply with standard interfaces (EIDE, ATA, SATA, USB, Fiber Channel, SCSI)\n' +
      '* The hard drives, in these interfaces, is seen as an *array of logical blocks (512 bytes)*\n' +
      '* The device, in hardware, does the translation between the block # and the platter #, sector #, track #, etc.\n' +
      '* This is good:\n' +
      '      * The kernel code to access the disk is straightforward\n' +
      '      * The controller can do a lot of work, e.g., transparently hiding bad blocks\n' +
      '      \n' +
      '### Hard Drive Performance\n' +
      '* Hard drives are **slow**\n' +
      '* Data request performance depends on three steps\n' +
      '      * **Seek** – moving the disk arm to the correct cylinder; Depends on how fast disk arm can move (increasing very slowly over\n' +
      'the years)\n' +
      '      * **Rotation** – waiting for the sector to rotate under the head; Depends on rotation rate of disk (increasing slowly over the years)\n' +
      '      * **Transfer** – transferring data from surface into disk controller electronics, sending it back to the host; Depends on density (increasing rapidly-ish over the years)\n' +
      '* **We must minimize seek and rotation time**; In current hard drives they are of the same order of magnitude\n' +
      '\n' +
      '## Disk Scheduling\n' +
      'Given the current position of the head (cylinder/track) and given a list of blocks to read, which block should we read next?\n' +
      '\n' +
      '### FIFO\n' +
      '* Simplest policy is First in, First Out\n' +
      '\n' +
      '### Shortest Seek Time First (SSTF)\n' +
      '* Select the request that\'s closest to the current head position\n' +
      '* Always goes to the request in the *nearest track*\n' +
      '* SSTF may cause **starvation**\n' +
      '* One solution is to used the **elevator algorithm (SCAN, F-SCAN)**\n' +
      '      * The head just goes through the tracks back and forth and serves requests as they come\n' +
      '      * Requests that come it for the current track may have to wait until the “elevator” comes back\n' +
      '      * But this is really not good to minimize request service time\n' +
      '      \n' +
      '### The Ultimate Algorithm: Shortest Access Time First (SATF)\n' +
      '* Account for *seek time* **and** *rotation time* to pick the next request\n' +
      '* All these algorithms are implemented in the *disk controller*\n' +
      '\n' +
      '## RAID\n' +
      '* **Disks are unreliable, slow, and cheap**\n' +
      '* Simple idea: let\'s use redundancy\n' +
      '      * Increases reliability (if one fails, you have another one)\n' +
      '      * Increases speed (Aggregate disk bandwith if data is split across disks)\n' +
      '* **Redundant Array of Independent Disks**\n' +
      '      * The OS can implement it with multiple bus-attached disks\n' +
      '      * An intelligent RAID controller in hardware\n' +
      '      * A “RAID array” as a stand-alone box\n' +
      '      \n' +
      '### RAID Techniques\n' +
      '* **Data Mirroring**\n' +
      '        * Keep the same data on multiple disks\n' +
      '        * Every write is to each mirror, whicht akes time\n' +
      '* **Data Striping**\n' +
      '        * Keep data split across multiple disks to allow parallel reads\n' +
      '* **Parity Bits**\n' +
      '        * Keep information from which to reconstruct lost bits due to a drive failing\n' +
      '* Combination of these techniques are called "levels" (RAID levels)\n' +
      '\n' +
      '#### RAID 0\n' +
      '* Data is striped across multiple disks\n' +
      '      * Using a fixed strip size\n' +
      '* Gives the illusion of a larger disk with high bandwidth when reading/writing a file\n' +
      '      * Accessing a single strip is not any faster\n' +
      '* Improves performance, but not reliability\n' +
      '* Useful for high-performance applications\n' +
      '\n' +
      '![](@attachment/ics332-13-2.png)\n' +
      '\n' +
      '#### RAID 1\n' +
      '* Mirroring (also called shadowing)\n' +
      '* Write every written byte to 2 disks\n' +
      '      * Uses twice as many disks as RAID 0\n' +
      '* Reliability is ensured unless you have (extremely unlikely) simultaneous failures \n' +
      '* Performance can be boosted by reading from the disk with the fastest seek time\n' +
      '      * The one with the arm the closest to the target cylinder\n' +
      '\n' +
      '![](@attachment/ics332-13-3.png)\n' +
      '\n' +
      '#### RAID 3\n' +
      '* **Bit-interleaved parity**\n' +
      '      * Each write goes to all disks, with each disk storing one bit\n' +
      '      * A parity bit is computed, stored, and used for data recovery\n' +
      '* Example with 4 disks an 1 parity disk\n' +
      '      * Say you store bits 0 1 1 0 on the 4 disks\n' +
      '      * The parity bit stores the XOR of those bits: (((0 xor 1) xor 1) xor 0) = 0\n' +
      '      * Say you lose one bit: 0 ? 1 0\n' +
      '      * You can XOR the remaining bits with the parity bit to recover the lost bit: (((0 xor 0) xor 1) xor 0) = 1\n' +
      '      * Say you lose a different bit: 0 1 1 ?\n' +
      '      * The XOR still works: (((0 xor 1) xor 1) xor 0) = 0\n' +
      '* Bit-level striping increases performance\n' +
      '* XOR overhead for each write (done in hardware)\n' +
      '* Time to recovery is long (a bunch of XOR’s)\n' +
      '\n' +
      '#### RAID 4, 5, and 6\n' +
      '* RAID 4: Basically like RAID 3, but interleaving it with strips\n' +
      '      * A small read will involve a single disk\n' +
      '* RAID 5: Like RAID 4, but parity is spread all over the disks as opposed to having just one parity disk:\n' +
      '\n' +
      '![](@attachment/ics332-13-4.png)\n' +
      '\n' +
      '* RAID 6: like RAID 5, but allows simultaneous disk failures (rarely used)\n' +
      '\n' +
      '## OS Management\n' +
      'The OS is responsible for:\n' +
      '* Formatting the disk\n' +
      '* Botting from disk\n' +
      '* Bad-block recovery\n' +
      '\n' +
      '## Physical disk formatting\n' +
      '* Divides the disk into sectors\n' +
      '* Fills the disk with a special data structure for each sector\n' +
      '      * A header, a data area (512 bytes), and a trailer\n' +
      '* In the header and trailer is the sector number, and extra bits for error-correcting code (ECC)\n' +
      '      * The ECC data is updated by the disk controller on each write and checked on each read\n' +
      '      * If only a few bits of data have been corrupted, the controller can use the ECC to fix those bits\n' +
      '      * Otherwise the sector is now known as “bad” which is reported to the OS\n' +
      '* Typically all done at the factory before shipping\n' +
      '\n' +
      '## Logical formatting\n' +
      '* The OS first partitions the disk into one or more groups of cylinders: **the partitions**\n' +
      '* The OS then treats each partition as a separate disk\n' +
      '* Then, **file system** information is written to the partitions\n' +
      '\n' +
      '## Boot blocks\n' +
      '* There is a small ROM-stored bootstrap program\n' +
      '* This program reads and loads a full bootstrap stored on disk\n' +
      '* The full bootstrap is stored in the boot blocks at a fixed location on a boot disk/partition (**master boot record**)\n' +
      '* This program then loads the OS\n' +
      '\n' +
      '## Bad blocks\n' +
      '* Sometimes, data on the disk is corrupted and the ECC can’t fix it\n' +
      '* Errors occur due to:\n' +
      '      * Damage to the platter’s surface\n' +
      '      * Defect in the magnetic medium due to wear\n' +
      '      * Temporary mechanical error (e.g., head touching the platter)\n' +
      '      * Temporary thermal fluctuation\n' +
      '* The OS gets a notification\n' +
      '* Upon reboot, the disk controller can be told to replace a bad block by a spare: **sector sparing**\n' +
      '  * Each time the OS asks for the bad block, it is given the spare instead\n' +
      '  * The controller maintains an entire block map\n' +
      '* *Problem*: the OS’s view of disk locality may be very different from the physical locality\n' +
      '* Solution #1: Spares in each cylinders and a spare cylinder\n' +
      '      * Always try to find spares “close” to the bad block\n' +
      '* Solution #2: Shuffle sectors to bring the spare block next to the bad block\n' +
      '      * Called **sector splitting**\n' +
      '      \n' +
      '## Solid State Drives (SSDs)\n' +
      '* Purely based on solid-state memory\n' +
      '      * Flash-based: persistent but slower (common case)\n' +
      '      * DRAM-based: faster but volatile\n' +
      '\n' +
      '### Advantages\n' +
      '* No moving parts!\n' +
      '* Flash SSDs competitive vs. hard drives:\n' +
      '      * faster startups and reads\n' +
      '      * silent, low-heat, low-power\n' +
      '      * more reliable\n' +
      '      * less heavy\n' +
      '      * getting larger and cheaper, close to HDD\n' +
      '      * lower lifetime due to write wear off (used to be a big deal, but now ok especially for personal computers)\n' +
      '      * slower writes (????)\n' +
      '\n' +
      '### SSD Structure\n' +
      '* The **flash cell**\n' +
      '* The **page** (4 KiB)\n' +
      '* The **block** (128 pages: 512 KiB)\n' +
      '\n' +
      '#### SSDs have slow writes???\n' +
      '* SSD writes are considered slow because of **write amplification**: as time goes on, a write of x bytes of data in fact entails writing y > x bytes of data!!\n' +
      '* Reason:\n' +
      '      * The smallest unit that can be **read**: a 4-KiB page\n' +
      '      * The smallest unit that can be **erased**: a 512-KiB block\n' +
      '      \n' +
      '## Write Amplification Example\n' +
      '\n' +
      '![](@attachment/ics332-13-5.png)\n' +
      '\n' +
      '![](@attachment/ics332-13-6.png)\n' +
      '\n' +
      '![](@attachment/ics332-13-7.png)\n' +
      '\n' +
      '* To write 4KiB + 8KiB + 16KiB = 28KiB of application data, we had to write 4KiB + 8KiB + 24KiB = 36KiB of data to the SSD\n' +
      '* As the drive fills up and files get written/modified/deleted, writes end up amplified\n' +
      '* The controller keeps writing on the SSD until full, before it attempts any rewrite\n' +
      '* In the end, performance is still good relative to that of an HDD\n' +
      '* The OS can, in the background, clean up block with invalid pages so that they’re easily writable\n' +
      '\n' +
      '## SSDs vs HDDs\n' +
      '* SSDs have many advantages of HDDs\n' +
      '      * Random read latency much smaller\n' +
      '      * SSDs are great at parallel read/write\n' +
      '      * SSDs are great at small writes\n' +
      '      * SSDs are great for random access in general\n' +
      '      * Which is typically the bane of HDDs\n' +
      '      * they are getting cheaper\n' +
      '* Note that not all SSDs are made equal\n' +
      '\n' +
      '## Conclusion\n' +
      '* HDDs are slow, large, unreliable, and cheap\n' +
      '* Disk scheduling helps with performance\n' +
      '* Redundancy is a way to cope with slow and unreliable HDDS (RAID)\n' +
      '* SSDs provide a radically different approach that may replace HDDs in the future\n' +
      '* The OS is involved minimally in disk management functions as the drive controllers do most of the work\n' +
      '\n';
  const html10 = converter.makeHtml(note10);
  target.innerHTML += html10;

</script>

<script>
  renderMathInElement(document.body);
</script>
</body>
</html>
